\chapter{Data Reduction}
\label{chap:data_reduction}

\section{Open and view a dataset}
\label{sec:open_file}
  The first step to start with is to enter the number of a direct beam dataset in the "Open Number:" entry of the Files area and press enter. The program will now locate the file and open it.
  The Files area list will be populated with all files in your current proposal data folder and the plot windows should look similar to this:
  
  \begin{tabular}{ccc}
  \includegraphics[width=155pt]{screenshots/normalizemap1.png} &
  \includegraphics[width=155pt]{screenshots/normalizemap2.png} &\\
  Overview X-Y & Overview ToF-X & \\
  \includegraphics[width=145pt]{screenshots/normalize1.png} &
  \includegraphics[width=145pt]{screenshots/normalize2.png} &
  \includegraphics[width=145pt]{screenshots/normalize3.png} \\
  X-Projection & Y-Projection & Reflectivity
  \end{tabular}



\section{Go full-automatic: Reduction for dummies}
  For good quality data (enough intensity and narrow reflection) the program supports a fully automatized mode, where all reduction parameters are automatically calculated.
  This mode will be applied automatically when more than one dataset is selected at the File Open Dialog.
  The direct beam measurements have to have lower scan numbers than the actual measurements or need to be set in advance for this method to work.
  
  The automatic algorithm performs the same steps as described in section \ref{sec:quick_start}, while trying to guess the best parameters.
  The datasets are read one-by-one and, depending on the reflection angle, they are either set as direct beam or reflectivity data in the reduction list.
  Here is an example how the interface might look after the algorithm has finished:
  
  \includegraphics[width=460pt]{screenshots/overview.png}
  
  
  You can now scale individual datasets as described in \ref{sec:scaling}, if the stitching was not performed optimally.
  When satisfied with the result, you can save the data as described in the export section \ref{sec:export}.
  
\clearpage
\section{Quick start: Step-by-step standard reduction}
\label{sec:quick_start}
  For most datasets the reduction is done very similar to the fully automatized method but with more control by the user.
  Every dataset is examined by the operator to select the best extraction parameters. This description should work in
  almost all circumstances.
  
  \subsection{Step 1: Set direct beam runs}
    \begin{wrapfigure}[9]{r}{0.55\textwidth}
    \begin{tabular}{cc}
        Reflectivity before & Reflectivity after \\
      \includegraphics[width=115pt]{screenshots/normalize3.png} & \includegraphics[width=115pt]{screenshots/normalize_after.png}
    \end{tabular}     
    \end{wrapfigure}
  
        \textbf{Open your direct beam file} as described in section \ref{sec:open_file}. 
      Make sure the SANGLE-calc value shown in the overview tab is close to zero and that the X- and Y-projections show the correct regions indicated with vertical lines.
      Activate the \textbf{Set Normalization action} \icon{extractNormalization}, this will add the current dataset to the "Direct Beam" list, the "Direct Beam Runs:" label will show the number of the dataset and the reflectivity will show the normalized intensities, which should all be one. You can use the \textbf{Cut Points (L/R) action} \icon{cutPoints} here to already set reasonable parameters for the Cut Pts entries.
      Repeat this step for each direct beam measurement needed for your datasets.

  
  \subsection{Step 2: Define a suitable background- and y-region}
    \begin{wrapfigure}[16]{r}{0.6\textwidth}
    \centering
      X-projection with background region (green)\\
      \includegraphics[width=155pt]{screenshots/background.png}
    \begin{tabular}{cc}
        Y-projection of small sample & X-Y map \\
      \includegraphics[width=150pt]{screenshots/yregion.png} & \includegraphics[width=100pt]{screenshots/yregionmap.png}
    \end{tabular}     
    \end{wrapfigure}
  
    Although it is in principle possible to define the extraction and background region for each dataset separately, it is recommended to use the same parameters for all files.
    From this perspective it is often a good idea to start with the dataset with the highest incident angle, as the signal to background ration is the lowest there.
    To produce the best results you should select a large region (statistics), keeping enough distance from the reflected beam (especially when off-specular Bragg-sheets are present) and to not include regions where the background drops (shadowed by the left detector slit for example).
    The Y-region, shown in the Y-projection of the first (low Q) dataset, is often detected very well automatically.
    Just check that it fits to the reflected intensity area. 
    For very small samples it can sometimes make sense to manually restrain the area to the sample reflection using the right mouse button on X-Y map.
  
  \subsection{Step 3: Normalize to total reflection and add the first dataset}
    \begin{wrapfigure}[12]{r}{0.4\textwidth}
     \includegraphics[width=165pt]{screenshots/totalreflection.png} 
    \end{wrapfigure}
    Go to your dataset starting at the lowest \Qz value, remove points from the low \Qz region, which are not reasonable with the \textbf{Cut Pts parameters}:
    \includegraphics[width=115pt]{screenshots/cutpoints.png} (can be done automatically with the \textbf{Cut Points (L/R)} \icon{cutPoints} action if not already performed after direct beam selection). Than activate the \textbf{Set Scaling action} \icon{totalReflection} to normalize the total reflection to one.
    This should now look like the image on the right.
    Next add the dataset to the refinement list using the \textbf{Keep Item in List action} \icon{addRef}, copying the current parameters to the list.
    This will automatically switch off the \textbf{Automatic Y Limits} \icon{limitYauto}, so all datasets will be reduced with the same Y-range.
    This is important for the high \Qz region as the background often inhibits a good automatic detection of the Y-region.
    
  
  \subsection{Step 4: Add additional datasets and stitch them together}
    \begin{wrapfigure}[12]{r}{0.4\textwidth}
    \centering
     \includegraphics[width=175pt]{screenshots/stitching1.png} 
    \end{wrapfigure}
    Now you can continue adding each subsequent dataset one after another.
    If nothing goes wrong, the only thing that needs to be changed from dataset to dataset is the \textbf{Scaling} parameter.
    If the scaling of subsequent datasets does not fit, activate the \textbf{Set Scaling action} \icon{totalReflection} again. 
    This fits a polynomial to the logarithmic data of both adjacent datasets including a scaling factor for the second, which is than used for the scaling after the fit.
    The error weighted $\chi^2$ used for this refinement is:
    \begin{eqnarray*}
      \chi_{stitch}^2 =& \underset{DS1}{\sum} \frac{(\log(I_i)-p(Q_i))^2}{(\delta{}I_i/I_i)^2} + \underset{DS2}{\sum} \frac{(\log(I_j\cdot scale)-p(Q_j))^2}{(\delta{}I_j/I_j)^2} \\
      \text{with } p(Q) =& a\cdot Q^2 + b\cdot Q +c \text{ for polynom order 3}
    \end{eqnarray*}
    The resulting fit function is shown in the reflectivity plot together with the scaled data as can be seen in the figure on the right.
    For some datasets with very sharp features like multilayer Bragg-peaks this method will probably not work, in those cases you need to change the \textbf{Scale 10\^} parameter manually until the datasets fit together nicely.
    For polarized measurements it can sometimes be helpful to switch back and forth between different polarization states as the variation in contrast can lead to smooth transitions, where the other state has a sharp feature.
    Now add the dataset to the reduction list with \textbf{Keep Item in List action} \icon{addRef} again and repeat the procedure for all datasets belonging to this measurement.


  \subsection{Step 5: Refine the reflectivity scaling and cutting}
  \label{sec:scaling}
    \begin{wrapfigure}[10]{r}{0.33\textwidth}
    \centering
    \begin{tabular}{cc}
     \includegraphics[width=145pt]{screenshots/cleanpoints.png} 
    \end{tabular}
    \end{wrapfigure}
    When all datasets of one measurement have been added, as can be seen in the image on the right, you can try to improve the scaling of the different parts, if needed, and change the cutting parameters.
    To change the scaling of one dataset you can either change the value of the \textbf{I0 column} entry in the reduction list or move the mouse \textbf{cursor on top of the curve} you want to scale and \textbf{move the mouse wheel}.
    To remove unwanted point you need to change the values of the \textbf{NL} and \textbf{NR column} entries as they define the number of points cut from the low- and hight-Q side respectively.
    If the number of time of flight channels in the dataset is larger than the wavelength window it is possible that large values are needed (<=60) to see changes in the dataset. 
    (Removing of overlapping points with low statistics can be done with \textbf{Strip Overlap} \icon{stripOverlap}.)
    
\subsection{Step 6: Export your data}
  \label{sec:export}
    \begin{wrapfigure}[14]{r}{0.35\textwidth}
    \centering
     \includegraphics[width=150pt]{screenshots/reduced.png}
    \end{wrapfigure}
    Now you are ready to export you reflectivity! 
    Activate the \textbf{Reduce... action} \icon{reduce} from the menu, toolbar or the button below the reduction list.
    The reduce dialog has several options for the export of the dataset.
    You can select which reductions should be stored, choose the spin states to export and define which data formats should be created.
    As a default, the specular reflectivity of all available channels will be exported to separate ASCII files and a dialog with a plot of the resulting data will be shown afterwards.
    Additional output options are a combined ASCII file containing all states, a matlab or numpy datafile for later processing, a Gnuplot script and image file to plot the ASCII data and a GenX reflectivity modeling template already containing the measured data.
    If you want to send the resulting data to your email address you can use the \textbf{Email Results} tab to enter your address and select which and if the data should be send after the export.
    
    

\section{Examples}
  This section will give three example datasets, which you can use to try the reduction yourself and compare the result with the images in this manual.
  
\section{Common complications to be aware of}
  \begin{description}
   \item[Bend/Twisted samples] Samples which do not have a flat surface can produce "fan"-like or split reflections. In these cases the peak fitting can result in different extraction areas for different incident angles. Use a small X window and/or the \textbf{"Fan" Reflection} option (see section \ref{sec:fan}) from the \textbf{Reflectivity Extraction (Advanced)} tab and define the reflection position manually.
   
   \item[Runs don't fit together] This often is caused by one of two reasons: One possibility is that there are very sharp features in the reflectivity which are measured differently for the different angles, as the resolution changes with the angle. In this case the only possibility is to try to merge the datasets manually as good as possible. If the measurement is polarized, take a look at the other spin state, sometimes the feature won't be as pronounced and make it easer to find the scaling factor. The other possibility is that the direct beam position, which should be measured before the experiment and written to the datafile, is not correct. This can be checked with the according direct beam run. If the \textbf{SANGLE-calc} value for the direct beam run is not close to zero you can use the \textbf{Adjust Direct Beam action} \icon{tthZero} to overwrite the values read from the datafile. Afterwards the it should be possible to extract the reflectivity normally. Don't forget to reset this overwrite afterwards using \textbf{Clear Overwrite} (see section \ref{sec:overwrite}).
  \end{description}

